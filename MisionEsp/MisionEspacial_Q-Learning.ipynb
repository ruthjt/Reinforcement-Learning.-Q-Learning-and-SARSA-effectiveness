{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba8399b4-8b26-4895-b38a-ba8d8554b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from enum import Enum\n",
    "import pygame\n",
    "import sys\n",
    "from os import path\n",
    "\n",
    "class NaveAction(Enum):\n",
    "    izq = 0\n",
    "    abajo = 1\n",
    "    dcha = 2\n",
    "    arriba = 3\n",
    "\n",
    "class MisionEspacial:\n",
    "    def __init__(self, grid_rows=4, grid_cols=5, fps=1):\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.reset()\n",
    "        self.fps = fps\n",
    "        self.last_action = ''\n",
    "        self._init_pygame()\n",
    "\n",
    "    def _init_pygame(self):\n",
    "        pygame.init()\n",
    "        pygame.display.init()\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.action_font = pygame.font.SysFont(\"Calibre\", 30)\n",
    "        self.action_info_height = self.action_font.get_height()\n",
    "        self.cell_height = 64\n",
    "        self.cell_width = 64\n",
    "        self.cell_size = (self.cell_width, self.cell_height)\n",
    "        self.window_size = (self.cell_width * self.grid_cols, self.cell_height * self.grid_rows + self.action_info_height)\n",
    "        self.window_surface = pygame.display.set_mode(self.window_size)\n",
    "        self.load_sprites()\n",
    "\n",
    "    def load_sprites(self):\n",
    "        file_name = path.join(\"nave.png\")\n",
    "        img = pygame.image.load(file_name)\n",
    "        self.nave_img = pygame.transform.scale(img, self.cell_size)\n",
    "        file_name = path.join(\"cielo.png\")\n",
    "        img = pygame.image.load(file_name)\n",
    "        self.fondo_img = pygame.transform.scale(img, self.cell_size)\n",
    "        file_name = path.join(\"saturno.png\")\n",
    "        img = pygame.image.load(file_name)\n",
    "        self.target_img = pygame.transform.scale(img, self.cell_size)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.agent_pos = [0, 0]\n",
    "        random.seed(seed)\n",
    "        self.target_pos = [\n",
    "            random.randint(1, self.grid_rows-1),\n",
    "            random.randint(1, self.grid_cols-1)\n",
    "        ]\n",
    "\n",
    "    def perform_action(self, nave_action: NaveAction) -> bool:\n",
    "        self.last_action = nave_action\n",
    "        if nave_action == NaveAction.izq and self.agent_pos[1] > 0:\n",
    "            self.agent_pos[1] -= 1\n",
    "        elif nave_action == NaveAction.dcha and self.agent_pos[1] < self.grid_cols-1:\n",
    "            self.agent_pos[1] += 1\n",
    "        elif nave_action == NaveAction.arriba and self.agent_pos[0] > 0:\n",
    "            self.agent_pos[0] -= 1\n",
    "        elif nave_action == NaveAction.abajo and self.agent_pos[0] < self.grid_rows-1:\n",
    "            self.agent_pos[0] += 1\n",
    "        return self.agent_pos == self.target_pos\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        self.window_surface.fill((0,0,0))\n",
    "        for r in range(self.grid_rows):\n",
    "            for c in range(self.grid_cols):\n",
    "                pos = (c * self.cell_width, r * self.cell_height)\n",
    "                if [r, c] == self.agent_pos:\n",
    "                    self.window_surface.blit(self.nave_img, pos)\n",
    "                elif [r, c] == self.target_pos:\n",
    "                    self.window_surface.blit(self.target_img, pos)\n",
    "                else:\n",
    "                    self.window_surface.blit(self.fondo_img, pos)\n",
    "        text_img = self.action_font.render(f'AcciÃ³n: {self.last_action}', True, (0,0,0), (255,255,255))\n",
    "        text_pos = (0, self.window_size[1] - self.action_info_height)\n",
    "        self.window_surface.blit(text_img, text_pos)\n",
    "        pygame.display.update()\n",
    "        self.clock.tick(self.fps)\n",
    "\n",
    "    def _process_events(self):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "            if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "\n",
    "class MisionEspacialEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], 'render_fps': 4}\n",
    "\n",
    "    def __init__(self, grid_rows=4, grid_cols=5, render_mode=None):\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.render_mode = render_mode\n",
    "        self.mision_espacial = MisionEspacial(grid_rows=grid_rows, grid_cols=grid_cols, fps=self.metadata['render_fps'])\n",
    "        self.action_space = spaces.Discrete(len(NaveAction))\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=np.array([self.grid_rows-1, self.grid_cols-1, self.grid_rows-1, self.grid_cols-1]),\n",
    "            shape=(4,),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.mision_espacial.reset(seed=seed)\n",
    "        obs = np.concatenate((self.mision_espacial.agent_pos, self.mision_espacial.target_pos))\n",
    "        info = {}\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        target_reached = self.mision_espacial.perform_action(NaveAction(action))\n",
    "        reward = 0\n",
    "        terminated = False\n",
    "        if target_reached:\n",
    "            reward = 1\n",
    "            terminated = True\n",
    "        obs = np.concatenate((self.mision_espacial.agent_pos, self.mision_espacial.target_pos))\n",
    "        info = {}\n",
    "        if self.render_mode == 'human':\n",
    "            print(NaveAction(action))\n",
    "            self.render()\n",
    "        return obs, reward, terminated, False, info\n",
    "\n",
    "    def render(self):\n",
    "        self.mision_espacial.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03dd304b-79c9-47d8-a542-2f37bc4a6137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruthjorganestorres/anaconda3/envs/gym/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be int32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ruthjorganestorres/anaconda3/envs/gym/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaveAction.arriba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruthjorganestorres/anaconda3/envs/gym/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/ruthjorganestorres/anaconda3/envs/gym/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be int32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ruthjorganestorres/anaconda3/envs/gym/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaveAction.arriba\n",
      "NaveAction.abajo\n",
      "NaveAction.arriba\n",
      "NaveAction.abajo\n",
      "NaveAction.arriba\n",
      "NaveAction.arriba\n",
      "NaveAction.izq\n",
      "NaveAction.dcha\n",
      "NaveAction.dcha\n",
      "NaveAction.abajo\n",
      "NaveAction.dcha\n",
      "NaveAction.abajo\n",
      "NaveAction.izq\n",
      "NaveAction.dcha\n",
      "NaveAction.izq\n",
      "NaveAction.izq\n",
      "NaveAction.arriba\n",
      "NaveAction.arriba\n",
      "NaveAction.izq\n",
      "NaveAction.arriba\n",
      "NaveAction.dcha\n",
      "NaveAction.izq\n",
      "NaveAction.dcha\n",
      "NaveAction.arriba\n",
      "NaveAction.abajo\n",
      "NaveAction.dcha\n",
      "NaveAction.abajo\n",
      "NaveAction.abajo\n",
      "NaveAction.izq\n",
      "NaveAction.arriba\n",
      "NaveAction.arriba\n",
      "NaveAction.izq\n",
      "NaveAction.arriba\n",
      "NaveAction.abajo\n",
      "NaveAction.izq\n",
      "NaveAction.arriba\n",
      "NaveAction.arriba\n",
      "NaveAction.abajo\n",
      "NaveAction.arriba\n",
      "NaveAction.arriba\n",
      "NaveAction.abajo\n",
      "NaveAction.abajo\n",
      "NaveAction.dcha\n",
      "NaveAction.izq\n",
      "NaveAction.arriba\n",
      "NaveAction.dcha\n",
      "NaveAction.izq\n",
      "NaveAction.dcha\n",
      "NaveAction.izq\n",
      "NaveAction.abajo\n",
      "NaveAction.izq\n",
      "NaveAction.dcha\n",
      "NaveAction.abajo\n",
      "NaveAction.dcha\n",
      "NaveAction.abajo\n",
      "NaveAction.arriba\n",
      "NaveAction.dcha\n",
      "NaveAction.dcha\n",
      "NaveAction.arriba\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='v0_mision_espacial-v0',  \n",
    "    entry_point='__main__:MisionEspacialEnv',  \n",
    ")\n",
    "\n",
    "env = gym.make('v0_mision_espacial-v0', render_mode='human')\n",
    "obs = env.reset()[0]\n",
    "while True:\n",
    "    rand_action = env.action_space.sample()\n",
    "    obs, reward, terminated, _, _ = env.step(rand_action)\n",
    "    if terminated:\n",
    "        break\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84465fe-6336-489e-98a5-bf4db1b922a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruthjorganestorres/anaconda3/envs/gym/lib/python3.9/site-packages/gymnasium/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment v0_mision_espacial-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from enum import Enum\n",
    "import pygame\n",
    "import sys\n",
    "from os import path\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register\n",
    "import numpy as np\n",
    "\n",
    "# Acciones posibles que puede hacer la nave:\n",
    "class NaveAction(Enum):\n",
    "    izq = 0\n",
    "    abajo = 1\n",
    "    dcha = 2\n",
    "    arriba = 3\n",
    "\n",
    "# Distintos elementos (casillas) que componen el juego\n",
    "class Etiquetas(Enum):\n",
    "    agua = 0\n",
    "    nave = 1\n",
    "    objetivo = 2\n",
    "\n",
    "# Al imprimir un objeto de la clase Etiquetas, nos muestra solo el primer caracter del nombre del elemento\n",
    "def __str__(self):\n",
    "    return self.name[:1]\n",
    "\n",
    "class MisionEspacialEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], 'render_fps': 4}\n",
    "\n",
    "    def __init__(self, grid_rows=4, grid_cols=5, render_mode=None, fps=4):\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.render_mode = render_mode\n",
    "        self.fps = fps\n",
    "        self.last_action = ''\n",
    "        self._init_pygame()\n",
    "        self.reset()\n",
    "\n",
    "        self.action_space = spaces.Discrete(len(NaveAction))\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=np.array([self.grid_rows-1, self.grid_cols-1, self.grid_rows-1, self.grid_cols-1]),\n",
    "            shape=(4,),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "\n",
    "    def _init_pygame(self):\n",
    "        pygame.init()\n",
    "        pygame.display.init()\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.action_font = pygame.font.SysFont(\"Calibre\", 30)\n",
    "        self.action_info_height = self.action_font.get_height()\n",
    "        self.cell_height = 64\n",
    "        self.cell_width = 64\n",
    "        self.cell_size = (self.cell_width, self.cell_height)\n",
    "        self.window_size = (self.cell_width * self.grid_cols, self.cell_height * self.grid_rows + self.action_info_height)\n",
    "        self.window_surface = pygame.display.set_mode(self.window_size)\n",
    "        self.load_sprites()\n",
    "\n",
    "    def load_sprites(self):\n",
    "        file_name = path.join(\"nave.png\")\n",
    "        img = pygame.image.load(file_name)\n",
    "        self.nave_img = pygame.transform.scale(img, self.cell_size)\n",
    "\n",
    "        file_name = path.join(\"cielo.png\")\n",
    "        img = pygame.image.load(file_name)\n",
    "        self.fondo_img = pygame.transform.scale(img, self.cell_size)\n",
    "\n",
    "        file_name = path.join(\"saturno.png\")\n",
    "        img = pygame.image.load(file_name)\n",
    "        self.objetivo_img = pygame.transform.scale(img, self.cell_size)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.agent_pos = [0, 0]\n",
    "        random.seed(seed)\n",
    "        self.target_pos = [\n",
    "            random.randint(1, self.grid_rows-1),\n",
    "            random.randint(1, self.grid_cols-1)\n",
    "        ]\n",
    "        obs = np.concatenate((self.agent_pos, self.target_pos))\n",
    "        return obs, {}\n",
    "\n",
    "    def perform_action(self, nave_action: NaveAction) -> bool:\n",
    "        self.last_action = nave_action\n",
    "        if nave_action == NaveAction.izq:\n",
    "            if self.agent_pos[1] > 0:\n",
    "                self.agent_pos[1] -= 1\n",
    "        elif nave_action == NaveAction.dcha:\n",
    "            if self.agent_pos[1] < self.grid_cols-1:\n",
    "                self.agent_pos[1] += 1\n",
    "        elif nave_action == NaveAction.arriba:\n",
    "            if self.agent_pos[0] > 0:\n",
    "                self.agent_pos[0] -= 1\n",
    "        elif nave_action == NaveAction.abajo:\n",
    "            if self.agent_pos[0] < self.grid_rows-1:\n",
    "                self.agent_pos[0] += 1\n",
    "        return self.agent_pos == self.target_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        target_reached = self.perform_action(NaveAction(action))\n",
    "        reward = 1 if target_reached else 0\n",
    "        terminated = target_reached\n",
    "        obs = np.concatenate((self.agent_pos, self.target_pos))\n",
    "        return obs, reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        self.window_surface.fill((0, 0, 0))\n",
    "        for r in range(self.grid_rows):\n",
    "            for c in range(self.grid_cols):\n",
    "                pos = (c * self.cell_width, r * self.cell_height)\n",
    "                if [r, c] == self.agent_pos:\n",
    "                    self.window_surface.blit(self.nave_img, pos)\n",
    "                elif [r, c] == self.target_pos:\n",
    "                    self.window_surface.blit(self.objetivo_img, pos)\n",
    "                else:\n",
    "                    self.window_surface.blit(self.fondo_img, pos)\n",
    "        text_img = self.action_font.render(f'Action: {self.last_action}', True, (0, 0, 0), (255, 255, 255))\n",
    "        text_pos = (0, self.window_size[1] - self.action_info_height)\n",
    "        self.window_surface.blit(text_img, text_pos)\n",
    "        pygame.display.update()\n",
    "        self.clock.tick(self.fps)\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n",
    "        sys.exit()\n",
    "\n",
    "# Registro del entorno personalizado\n",
    "register(\n",
    "    id='v0_mision_espacial-v0',\n",
    "    entry_point='__main__:MisionEspacialEnv',\n",
    ")\n",
    "\n",
    "# CreaciÃ³n del entorno\n",
    "env = gym.make('v0_mision_espacial-v0', render_mode='human')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e73f273c-8113-42d2-a384-a090a2a5ab1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 3-dimensional, but 10 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m epsilon:\n\u001b[0;32m---> 54\u001b[0m         action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurr_discrete_state\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(env\u001b[38;5;241m.\u001b[39maction_space)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 3-dimensional, but 10 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Assuming 'env' is imported and defined somewhere in your code\n",
    "# For example:\n",
    "# from my_environment import MyEnvironment\n",
    "# env = MyEnvironment()\n",
    "\n",
    "# Mocking 'env' for demonstration\n",
    "class MyEnvironment:\n",
    "    def __init__(self):\n",
    "        self.grid_rows = 10\n",
    "        self.grid_cols = 10\n",
    "        self.action_space = [0, 1, 2, 3]  # Example action space\n",
    "\n",
    "    def reset(self):\n",
    "        return np.zeros((self.grid_rows, self.grid_cols)), 0\n",
    "\n",
    "    def step(self, action):\n",
    "        return np.zeros((self.grid_rows, self.grid_cols)), 0, False, {}, {}\n",
    "\n",
    "# ConfiguraciÃ³n de hiperparÃ¡metros\n",
    "episodes = 10\n",
    "discount = 0.95\n",
    "episodes_display = 10\n",
    "learning_rate = 0.25\n",
    "epsilon = 0.2\n",
    "\n",
    "# Inicializar tabla Q con valores aleatorios\n",
    "env = MyEnvironment()  # Mock environment\n",
    "q_table = np.random.randn(env.grid_rows, env.grid_cols, len(env.action_space))\n",
    "\n",
    "# Almacenamiento de las recompensas totales por episodio\n",
    "ep_rewards = []\n",
    "ep_rewards_table = {'ep': [], 'avg': [], 'min': [], 'max': []}\n",
    "\n",
    "def discretised_state(state):\n",
    "    return tuple(state.astype(int))\n",
    "\n",
    "for episode in range(episodes):\n",
    "    episode_reward = 0\n",
    "    state_array, _ = env.reset()\n",
    "    curr_discrete_state = discretised_state(state_array)\n",
    "    done = False\n",
    "\n",
    "    if episode % episodes_display == 0:\n",
    "        render_state = True\n",
    "    else:\n",
    "        render_state = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[curr_discrete_state])\n",
    "        else:\n",
    "            action = random.choice(env.action_space)\n",
    "\n",
    "        new_state_array, reward, done, _, _ = env.step(action)\n",
    "        new_discrete_state = discretised_state(new_state_array)\n",
    "\n",
    "        if render_state:\n",
    "            print(state_array)  # Rendering the state\n",
    "\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[curr_discrete_state][:][:][action]\n",
    "            new_q = current_q + learning_rate * (reward + discount * max_future_q - current_q)\n",
    "            q_table[curr_discrete_state][:][:][action] = new_q\n",
    "\n",
    "        curr_discrete_state = new_discrete_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    ep_rewards.append(episode_reward)\n",
    "\n",
    "    if not episode % episodes_display:\n",
    "        avg_reward = sum(ep_rewards[-episodes_display:]) / episodes_display\n",
    "        ep_rewards_table['ep'].append(episode)\n",
    "        ep_rewards_table['avg'].append(avg_reward)\n",
    "        ep_rewards_table['min'].append(min(ep_rewards[-episodes_display:]))\n",
    "        ep_rewards_table['max'].append(max(ep_rewards[-episodes_display:]))\n",
    "        print(f\"Episode:{episode} avg:{avg_reward} min:{min(ep_rewards[-episodes_display:])} max:{max(ep_rewards[-episodes_display:])}\")\n",
    "\n",
    "# env.close()  # Uncomment if your environment has a close method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55f8af5-97b2-4b1e-a5f5-439c22d612f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 3-dimensional, but 10 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m epsilon:\n\u001b[0;32m---> 54\u001b[0m         action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurr_discrete_state\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(env\u001b[38;5;241m.\u001b[39maction_space)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 3-dimensional, but 10 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Assuming 'env' is imported and defined somewhere in your code\n",
    "# For example:\n",
    "# from my_environment import MyEnvironment\n",
    "# env = MyEnvironment()\n",
    "\n",
    "# Mocking 'env' for demonstration\n",
    "class MyEnvironment:\n",
    "    def __init__(self):\n",
    "        self.grid_rows = 10\n",
    "        self.grid_cols = 10\n",
    "        self.action_space = [0, 1, 2, 3]  # Example action space\n",
    "\n",
    "    def reset(self):\n",
    "        return np.zeros((self.grid_rows, self.grid_cols)), 0\n",
    "\n",
    "    def step(self, action):\n",
    "        return np.zeros((self.grid_rows, self.grid_cols)), 0, False, {}, {}\n",
    "\n",
    "# ConfiguraciÃ³n de hiperparÃ¡metros\n",
    "episodes = 10\n",
    "discount = 0.95\n",
    "episodes_display = 10\n",
    "learning_rate = 0.25\n",
    "epsilon = 0.2\n",
    "\n",
    "# Inicializar tabla Q con valores aleatorios\n",
    "env = MyEnvironment()  # Mock environment\n",
    "q_table = np.random.randn(env.grid_rows, env.grid_cols, len(env.action_space))\n",
    "\n",
    "# Almacenamiento de las recompensas totales por episodio\n",
    "ep_rewards = []\n",
    "ep_rewards_table = {'ep': [], 'avg': [], 'min': [], 'max': []}\n",
    "\n",
    "def discretised_state(state):\n",
    "    return tuple(state.astype(int))\n",
    "\n",
    "for episode in range(episodes):\n",
    "    episode_reward = 0\n",
    "    state_array, _ = env.reset()\n",
    "    curr_discrete_state = discretised_state(state_array)\n",
    "    done = False\n",
    "\n",
    "    if episode % episodes_display == 0:\n",
    "        render_state = True\n",
    "    else:\n",
    "        render_state = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[curr_discrete_state])\n",
    "        else:\n",
    "            action = random.choice(env.action_space)\n",
    "\n",
    "        new_state_array, reward, done, _, _ = env.step(action)\n",
    "        new_discrete_state = discretised_state(new_state_array)\n",
    "\n",
    "        if render_state:\n",
    "            print(state_array)  # Rendering the state\n",
    "\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[curr_discrete_state][:][action]\n",
    "            new_q = current_q + learning_rate * (reward + discount * max_future_q - current_q)\n",
    "            q_table[curr_discrete_state][:][action] = new_q\n",
    "\n",
    "        curr_discrete_state = new_discrete_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    ep_rewards.append(episode_reward)\n",
    "\n",
    "    if not episode % episodes_display:\n",
    "        avg_reward = sum(ep_rewards[-episodes_display:]) / episodes_display\n",
    "        ep_rewards_table['ep'].append(episode)\n",
    "        ep_rewards_table['avg'].append(avg_reward)\n",
    "        ep_rewards_table['min'].append(min(ep_rewards[-episodes_display:]))\n",
    "        ep_rewards_table['max'].append(max(ep_rewards[-episodes_display:]))\n",
    "        print(f\"Episode:{episode} avg:{avg_reward} min:{min(ep_rewards[-episodes_display:])} max:{max(ep_rewards[-episodes_display:])}\")\n",
    "\n",
    "# env.close()  # Uncomment if your environment has a close method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c3e3864-3b2a-47da-a8d4-0bd971cda47c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 avg:1.0 min:1 max:1\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruthjorganestorres/anaconda3/envs/gym/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# ConfiguraciÃ³n de hiperparÃ¡metros\n",
    "episodes = 10\n",
    "discount = 0.95\n",
    "episodes_display = 10\n",
    "learning_rate = 0.25\n",
    "epsilon = 0.2\n",
    "\n",
    "# Inicializar tabla Q con valores aleatorios\n",
    "q_table = np.random.randn(env.grid_rows, env.grid_cols, env.grid_rows, env.grid_cols, env.action_space.n)\n",
    "\n",
    "# Almacenamiento de las recompensas totales por episodio\n",
    "ep_rewards = []\n",
    "ep_rewards_table = {'ep': [], 'avg': [], 'min': [], 'max': []}\n",
    "\n",
    "def discretised_state(state):\n",
    "    return tuple(state.astype(int))\n",
    "\n",
    "for episode in range(episodes):\n",
    "    episode_reward = 0\n",
    "    state_array = env.reset()[0]\n",
    "    curr_discrete_state = discretised_state(state_array)\n",
    "    done = False\n",
    "    i = 0\n",
    "\n",
    "    if episode % episodes_display == 0:\n",
    "        render_state = True\n",
    "    else:\n",
    "        render_state = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[curr_discrete_state])\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "        new_state_array, reward, done, _, _ = env.step(action)\n",
    "        new_discrete_state = discretised_state(new_state_array)\n",
    "\n",
    "        if render_state:\n",
    "            env.render()\n",
    "\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[curr_discrete_state + (action,)]\n",
    "            new_q = current_q + learning_rate * (reward + discount * max_future_q - current_q)\n",
    "            q_table[curr_discrete_state + (action,)] = new_q\n",
    "\n",
    "        curr_discrete_state = new_discrete_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    ep_rewards.append(episode_reward)\n",
    "\n",
    "    if not episode % episodes_display:\n",
    "        avg_reward = sum(ep_rewards[-episodes_display:])/len(ep_rewards[-episodes_display:])\n",
    "        ep_rewards_table['ep'].append(episode)\n",
    "        ep_rewards_table['avg'].append(avg_reward)\n",
    "        ep_rewards_table['min'].append(min(ep_rewards[-episodes_display:]))\n",
    "        ep_rewards_table['max'].append(max(ep_rewards[-episodes_display:]))\n",
    "        print(f\"Episode:{episode} avg:{avg_reward} min:{min(ep_rewards[-episodes_display:])} max:{max(ep_rewards[-episodes_display:])}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
